{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq4vAHa4Yaua"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "73FOJnPVYn20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mecab-python3 unidic-lite POT"
      ],
      "metadata": {
        "id": "LeO7PJqMYocD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.word_vectors.200d.txt.bz2\n",
        "!bunzip2 jawiki.word_vectors.200d.txt.bz2"
      ],
      "metadata": {
        "id": "6F0Ujpi7Yryj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下がsentences.txtの内容です。ChatGPTに対して「類似文章検索のコードのテストのための文章を100個生成してください。」というというプロンプトを与えて生成しました。\n",
        "\n",
        "\n",
        "```\n",
        "昨日は近くの公園を散歩した。\n",
        "公園では犬を散歩させている人が多かった。\n",
        "昨日の午後、公園に行って読書をした。\n",
        "昨日は雨が降っていたが、傘を差して歩いた。\n",
        "雨の日は家で本を読むのが好きだ。\n",
        "曇りの日は少し憂鬱な気分になる。\n",
        "晴れた日は気持ちが明るくなる。\n",
        "快晴の空を見ると旅行に行きたくなる。\n",
        "夏休みには山に登るのが恒例だ。\n",
        "去年の夏は北海道へ旅行した。\n",
        "北海道の自然はとても美しい。\n",
        "自然の中にいると心が落ち着く。\n",
        "森林浴はストレス解消に効果があると言われている。\n",
        "ストレスが溜まったときは音楽を聴く。\n",
        "クラシック音楽を聴くと集中力が高まる。\n",
        "勉強するときは静かな音楽を流す。\n",
        "試験前には暗記カードを使って復習する。\n",
        "試験勉強は計画的に進める必要がある。\n",
        "勉強は短時間でも毎日継続することが大事。\n",
        "毎日少しずつ積み重ねるのが成功の秘訣だ。\n",
        "努力は報われると信じている。\n",
        "努力する人を見ると自分も頑張ろうと思う。\n",
        "モチベーションを保つのは難しいこともある。\n",
        "やる気が出ない日は無理をしないようにしている。\n",
        "休息も大切な時間のひとつだ。\n",
        "睡眠時間をしっかり確保するようにしている。\n",
        "健康のために毎朝ラジオ体操をしている。\n",
        "体調を崩す前に予防が大切だ。\n",
        "手洗いうがいは基本的な予防対策だ。\n",
        "外出先ではマスクをつけている。\n",
        "マスクをつけるのが日常になった。\n",
        "新しい生活様式に慣れてきた。\n",
        "リモートワークにはまだ慣れていない。\n",
        "家で仕事をするのは集中力が必要だ。\n",
        "作業環境を整えると効率が上がる。\n",
        "パソコンの前に長時間座ると疲れる。\n",
        "定期的にストレッチをすると体が楽になる。\n",
        "姿勢を正しく保つことを意識している。\n",
        "正しい姿勢は腰痛予防にも効果的だ。\n",
        "最近は腰痛改善のための椅子を購入した。\n",
        "快適な椅子に座ると仕事がはかどる。\n",
        "デスク周りを整理すると気分がすっきりする。\n",
        "整理整頓が苦手なので努力している。\n",
        "整理術の本を読んで試してみた。\n",
        "本を読むのが日課になっている。\n",
        "読書は想像力を豊かにする。\n",
        "小説を読むと登場人物に感情移入してしまう。\n",
        "推理小説を読むのが好きだ。\n",
        "ミステリー作品には予想外の展開がある。\n",
        "サスペンスドラマを見るとドキドキする。\n",
        "映画は週に1本くらいのペースで観ている。\n",
        "映画館で観る映画は迫力が違う。\n",
        "映画の感想を友達と話し合うのが楽しい。\n",
        "話すことで考えが整理されることがある。\n",
        "会話はコミュニケーションの基本だ。\n",
        "コミュニケーション能力を高めたい。\n",
        "話すだけでなく、聞く力も大切だと思う。\n",
        "相手の話を最後まで聞くようにしている。\n",
        "相手の立場を考えることが重要だ。\n",
        "共感することは信頼関係を築く第一歩だ。\n",
        "信頼は一朝一夕には築けない。\n",
        "時間をかけて関係を深めていきたい。\n",
        "人間関係には相性も関係している。\n",
        "自分と合う人と過ごすと安心できる。\n",
        "安心できる場所があると心が落ち着く。\n",
        "家は自分にとって一番落ち着ける場所だ。\n",
        "自宅で過ごす時間を充実させたい。\n",
        "休日は家でゆっくり映画を見るのが好き。\n",
        "休日に掃除をすると気分がリフレッシュする。\n",
        "掃除機の音を聞くとやる気が出る。\n",
        "音のある環境が落ち着くこともある。\n",
        "カフェの雑音が意外と集中できる。\n",
        "カフェで勉強するのが日課になっている。\n",
        "勉強には場所の雰囲気も大事だ。\n",
        "図書館の静けさは集中に適している。\n",
        "図書館には多くの資料が揃っている。\n",
        "資料を探すのが好きで、調べ物が趣味だ。\n",
        "新しい知識を得ると嬉しくなる。\n",
        "知識は人生を豊かにすると信じている。\n",
        "生涯学習の意識を持って生活している。\n",
        "毎日少しずつ成長していきたい。\n",
        "成長のためには挑戦が必要だ。\n",
        "新しいことに挑戦するのが好きだ。\n",
        "失敗を恐れずに行動したい。\n",
        "失敗から学ぶことが多いと感じる。\n",
        "経験は何よりの財産だと思う。\n",
        "経験値を積むことで自信がつく。\n",
        "自信は小さな成功の積み重ねから生まれる。\n",
        "成功体験を振り返ることで前向きになれる。\n",
        "前向きな気持ちで日々を過ごしたい。\n",
        "毎日を丁寧に生きることを意識している。\n",
        "小さな幸せを感じられるようになりたい。\n",
        "幸せは自分の中にあると思う。\n",
        "幸せについて考える時間が増えた。\n",
        "哲学的な問いに興味が出てきた。\n",
        "哲学の本を読むと考えが深まる。\n",
        "深く考えることで視野が広がる。\n",
        "視野を広げるために旅行したい。\n",
        "海外の文化に触れると刺激を受ける。\n",
        "異文化理解はこれからの時代に必要だ。\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uSPAfvtPbFjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "jawiki.word_vectors.200d.txtのロードに時間がかかるので、文章をベクトル化して保存しておきます。"
      ],
      "metadata": {
        "id": "P9K5jTuUZbuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "from gensim.models import KeyedVectors\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "def get_w(text, mt, wv):\n",
        "    kws = mt.parse(text).split()\n",
        "    w = np.array([np.array(wv[kw]) for kw in kws if kw in wv])\n",
        "    return w\n",
        "\n",
        "def main():\n",
        "    mt = MeCab.Tagger('-Owakati')\n",
        "    wv = KeyedVectors.load_word2vec_format('jawiki.word_vectors.200d.txt')\n",
        "\n",
        "    with open(\"sentences.txt\", \"r\") as f:\n",
        "        sentences = f.readlines()\n",
        "\n",
        "    def get_w_wrapper(s):\n",
        "        return get_w(s, mt, wv)\n",
        "\n",
        "    sentence_vecs = list(map(get_w_wrapper, sentences))\n",
        "\n",
        "    with open('sentence_vecs.pickle', mode='wb') as f:\n",
        "        pickle.dump(sentence_vecs, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "dq3ORIMlYsSt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import ot\n",
        "\n",
        "def get_z(w):\n",
        "    z = 0\n",
        "    for w_i in w:\n",
        "        z += np.linalg.norm(w_i)\n",
        "    return z\n",
        "\n",
        "\n",
        "def cos_sim(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def pathfinders_calc_similarity(sentence_vecs, combination):\n",
        "    w1 = sentence_vecs[combination[0]]\n",
        "    w2 = sentence_vecs[combination[1]]\n",
        "\n",
        "    z1 = get_z(w1)\n",
        "    z2 = get_z(w2)\n",
        "\n",
        "    m1 = [np.linalg.norm(w1_i) / z1 for w1_i in w1]\n",
        "    m2 = [np.linalg.norm(w2_i) / z2 for w2_i in w2]\n",
        "\n",
        "    # Compute cost matrix C\n",
        "    c = []\n",
        "    for w1_i in w1:\n",
        "        c.append([1 - cos_sim(np.array(w1_i), np.array(w2_j)) for w2_j in w2])\n",
        "\n",
        "    # Show the result\n",
        "    return ot.emd2(m1, m2, c)\n",
        "\n",
        "def followers_calc_similarity(sentence_vecs, combination):\n",
        "    w1 = sentence_vecs[combination[0]]\n",
        "    w2 = sentence_vecs[combination[1]]\n",
        "\n",
        "    w1_norm = np.linalg.norm(w1, axis=1)\n",
        "    m1 = w1_norm / w1_norm.sum()\n",
        "\n",
        "    w2_norm = np.linalg.norm(w2, axis=1)\n",
        "    m2 = w2_norm / sum(w2_norm)\n",
        "\n",
        "    # Compute cost matrix C\n",
        "    w_dot = np.dot(w1, w2.T)\n",
        "    w_norm = np.outer(w1_norm, w2_norm.T)\n",
        "    c = 1 - w_dot / w_norm\n",
        "\n",
        "    # Show the result\n",
        "    return ot.emd2(m1, m2, c)\n",
        "\n",
        "def measure_time(calc_similarity, sentence_vecs, combinations):\n",
        "    def calc_similarity_wrapper(combination):\n",
        "        return calc_similarity(sentence_vecs, combination)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    result = list(map(calc_similarity_wrapper, combinations))\n",
        "    end = time.perf_counter()\n",
        "\n",
        "    print('{:.10f}'.format((end - start)))\n",
        "    # print(result)\n",
        "\n",
        "with open('sentence_vecs.pickle', mode='br') as f:\n",
        "    sentence_vecs = pickle.load(f)\n",
        "combinations = list(itertools.combinations(range(len(sentence_vecs)), 2))\n",
        "\n",
        "measure_time(pathfinders_calc_similarity, sentence_vecs, combinations)\n",
        "measure_time(followers_calc_similarity, sentence_vecs, combinations)"
      ],
      "metadata": {
        "id": "7M1NS7hVZ84K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pathfinders_calc_similarity: 11.4994708340sec<br>\n",
        "followers_calc_similarity: 1.9490371520sec\n",
        "\n",
        "十倍以上の高速化が確認できます。"
      ],
      "metadata": {
        "id": "T1GBbSwMcTxL"
      }
    }
  ]
}